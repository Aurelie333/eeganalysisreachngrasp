{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"CNN MI raw CV.ipynb","provenance":[{"file_id":"1_7zQ0AJ6wAe8wGQFaEbERVeCYmSaghPx","timestamp":1623757356991}],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WVeAI8UjLhC1","executionInfo":{"status":"ok","timestamp":1625085335447,"user_tz":-120,"elapsed":19442,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"1d7b473c-ac90-4bce-a8e3-7828449b66b7"},"source":["from google.colab import drive \n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qx8c9p89ZeX_","executionInfo":{"status":"ok","timestamp":1625085517202,"user_tz":-120,"elapsed":284,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"cfbd3b13-2ddb-4971-cc48-c0658c2a36f7"},"source":["%cd /content/drive/My\\ Drive/Colab Notebooks"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L1Hs9mr1S62N"},"source":["BIG_TITLE=\"CNN raw movement intention\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2bUTWjCLi4J"},"source":["### TIME WINDOWS\n","w1=768\n","w2=0\n","W1=w1\n","W2=w2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TahQvg_LOMpE"},"source":["### LIBRARIES\n","\n","import tensorflow as tf\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy\n","import datetime\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Embedding\n","from keras.layers import BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n","from keras.optimizers import Adam\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras import layers\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import confusion_matrix\n","import time\n","from sklearn.neural_network import MLPClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","import pandas\n","import os\n","\n","# Remove some unwanted warnings\n","import logging\n","logging.getLogger('tensorflow').disabled = True \n","from scipy.signal import butter, lfilter\n","import pywt\n","import pandas as pd\n","import numpy as np\n","import random\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy\n","import scipy\n","import scipy.signal\n","from scipy import fft\n","### LIBRARIES\n","\n","import tensorflow as tf\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy\n","import datetime\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Embedding\n","from keras.layers import BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n","from keras.optimizers import Adam\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras import layers\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import confusion_matrix\n","import time\n","from sklearn.neural_network import MLPClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","import scipy\n","import pandas\n","import os\n","\n","import os\n","import sys\n","\n","import pandas as pd\n","import pandas\n","import numpy as np\n","\n","import pywt\n","import scipy.io as spio\n","from scipy.stats import entropy\n","from collections import Counter\n","\n","from sklearn import svm\n","from sklearn.preprocessing import normalize\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","import timeit\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Embedding\n","from keras.layers import LSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n","from keras.optimizers import Adam\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras import layers\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import confusion_matrix\n","from numpy import hstack, vstack, dstack\n","import numpy as np\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWmWx0OFLf4J"},"source":["def load_signal(k):  #where k is the file name\n","    data = sio.loadmat(str(k))  \n","    ###deleting EOGs\n","    signal_raw= numpy.delete(data[\"signal\"], obj= [63,62,61,60,59,58], axis=0)\n","    ###deleting 0s:\n","    signal_raw_trans= numpy.transpose(signal_raw)\n","    ind=[]\n","    for cx, c in enumerate(signal_raw_trans):\n","        if len(c) -  numpy.count_nonzero(c) <=5:\n","            continue\n","        else:\n","            ind.append(cx)\n","    g= numpy.delete(signal_raw_trans, obj= ind, axis=0)\n","    signal_raw_trans_clean=numpy.transpose(g)\n","    \n","    ###clean memory\n","    del data\n","    del signal_raw\n","    del signal_raw_trans\n","    del g\n","\n","    return(signal_raw_trans_clean)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bknjnwe2Lf4J"},"source":["### cleaned code names\n","def cleaned_code_names(k):# k=file name\n","    data = sio.loadmat(str(k)) \n","    events= data[\"header\"][\"event_codes\"]\n","    EVENTS=[]\n","    for i in events:\n","        for e in i:\n","            for j in e:\n","                for h in j:\n","                    if len(str(h))>2:\n","                        EVENTS.append(h)\n","    EVENTS=numpy.array(EVENTS)\n","    \n","    del data\n","    del events\n","    \n","    return EVENTS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYG7Knu7Lf4K"},"source":["### cleaned event code\n","def event_Code(k): # k=file name\n","    data = sio.loadmat(str(k)) \n","    \n","    labels= str(data[\"events\"][\"codes\"]).split()\n","    \n","    l=[]\n","    for g in labels:\n","        for i in g:\n","            if i.isnumeric():\n","                l.append(i)\n","        else:\n","            l.append(\",\")\n","    #cleaning labels \n","    l2=[]\n","    g=\"\"\n","    for j in l:\n","        if j.isnumeric():\n","            g+=j\n","        elif len(g)<1:\n","            continue\n","        else:\n","            l2.append(int(g))\n","            g=\"\"\n","    \n","    del data\n","    del labels\n","    del l\n","    del g\n","    \n","    return l2\n","\n","### cleaned event position\n","def event_Position(k):# k=file name\n","    data = sio.loadmat(str(k)) \n","    indices_signal_matrix= str(data[\"events\"][\"positions\"]).split()\n","    #cleaning indices_signal_matrix \n","    l3=[]\n","    for g in indices_signal_matrix:\n","        for i in g:\n","            if i.isnumeric():\n","                l3.append(i)\n","        else:\n","            l3.append(\",\")\n","\n","    l4=[]\n","    g=\"\"\n","    for j in l3:\n","        if j.isnumeric():\n","            g+=j\n","        elif len(g)<1:\n","            continue\n","        else:\n","            l4.append(int(g))\n","            g=\"\"\n","    l4=l4[:-1]#array of i\n","    \n","    del data\n","    del indices_signal_matrix\n","    del l3\n","    del g\n","    \n","    return l4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuIgNuzqLf4K"},"source":["#indexes of each events within signals\n","def indexes_event_signals(k):\n","    data = sio.loadmat(str(k)) \n","    EVENTS = cleaned_code_names(k)\n","    event_codes= event_Code(k)\n","    dico_indexes={}\n","\n","    for event in EVENTS:\n","        indexes_=[]\n","        for ig, g in enumerate(event_codes):\n","            if g == event:\n","                indexes_.append(ig)\n","        dico_indexes[event]= indexes_\n","    \n","    del data\n","    del EVENTS\n","    del event_codes\n","    del indexes_\n","    \n","    return dico_indexes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz5V-Na8Lf4L"},"source":["def start_indexes(k, event):# k= file name, starting_event= code of grasp onset, ending_event:code of grasp offset\n","    indexes=indexes_event_signals(k)\n","    event_positions= event_Position(k)\n","    event_indexes=[]\n","    \n","    for h in indexes[event]:\n","        event_indexes.append(event_positions[h])\n","    \n","    del indexes\n","    del event_positions\n","    \n","    return event_indexes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JoSEefILLf4L"},"source":["#signals of movement types (signal of split() function)\n","\n","\n","def EVENT_SIGNALS(k, EVENT, W1, W2): #starting_event= starting event of the movement, ending_event:ending event of the movement\n","    #window_size2=int(window_size/2)\n","    signal= load_signal(str(k)) \n","    event_indexs= start_indexes(str(k), EVENT)\n","\n","    movement_signals=[]\n","    #print(event_indexs)\n","    for i in event_indexs:#\n","        for channel in signal:\n","            i= int(i)\n","            #print(i+(window_size2))\n","            if i-W1>0:\n","                movement_signals.append(numpy.array(channel[i-W1:i+W2]))# window size would be are most litte event, event would be the ending event\n","            #print(len(channel[i-window_size2:i+(window_size2+1)]))\n","            else:\n","                continue\n","    del signal\n","    del event_indexs\n","    \n","    return movement_signals\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"31I5vHdkLf4M"},"source":["def create_tensor(data,W1:int, W2,nb_channels:int):\n","    window_size= W1+W2\n","    positions_split=[]\n","\n","    for j in range(0,len(data),nb_channels):\n","        positions_split.append(j)\n","\n","    positions_split.append(len(data))\n","    #print(positions_split)\n","\n","    all_events=[]\n","    for i in range(0,len(positions_split)-1):\n","        #print(i)\n","        one_event=[]\n","        for h in range(positions_split[i],positions_split[i+1]):\n","            #print(h)\n","            data_shaped=data[h]\n","            if (len(one_event)==0.0):\n","                one_event=data_shaped\n","            else:\n","                one_event=numpy.vstack((one_event, data_shaped))\n","        if (len(all_events)==0.0):\n","                all_events=one_event.reshape(1,nb_channels,window_size)\n","        else:\n","                all_events=numpy.vstack((all_events, one_event.reshape(1,nb_channels,window_size)))\n","        #print(all_events.shape)\n","    return(all_events)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDsD4BdnLf4M"},"source":["import random\n","import numpy as np\n","\n","def random_shuffle(final_TENSOR, labels):\n","    random_indexes_tens= random.sample(range(0, final_TENSOR.shape[0]), final_TENSOR.shape[0])\n","    event_tensors= [t for t in range(final_TENSOR.shape[0])]#indexes of tensor\n","    \n","    #print(random_indexes_tens)\n","    #print(event_tensors)\n","\n","    SHUFFLED_TENSOR=[] \n","    LABELS_SHUFFLED=[]\n","    \n","    for i in event_tensors:\n","        target_position=random_indexes_tens[i]\n","        SHUFFLED_TENSOR.append(final_TENSOR[target_position,:,:])\n","        LABELS_SHUFFLED.append(labels[target_position])\n","\n","    SHUFFLED_TENSOR=numpy.array(SHUFFLED_TENSOR)\n","    #SHUFFLED_TENSOR= np.transpose(SHUFFLED_TENSOR, (1,2,0))\n","    return SHUFFLED_TENSOR, LABELS_SHUFFLED"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ONjACjPLf4N"},"source":["# Remove some unwanted warnings\n","import logging\n","logging.getLogger('tensorflow').disabled = True \n","\n","def add_data(W1:int, W2:int,k:str,event_label:str,EVENT,nb_channels:int):\n","    #window_size=W1+W2\n","    partial_TENSOR=[]\n","    partial_LABEL=[]\n","    time00 = datetime.datetime.now()\n","    #for i in k:\n","    time1 = datetime.datetime.now()\n","    file='G'+k+'.mat'\n","    print('processing '+file)\n","    data=EVENT_SIGNALS(k=file, EVENT=EVENT, W1=W1, W2=W2 )\n","    data_transformed=create_tensor(data=data,W1=W1, W2=W2,nb_channels=nb_channels)\n","    label= event_label*data_transformed.shape[0]\n","    \n","    if (len(partial_TENSOR)==0.0):\n","        partial_TENSOR=data_transformed\n","        partial_LABEL=label\n","    else:\n","        partial_TENSOR=numpy.vstack((partial_TENSOR, data_transformed))\n","        partial_LABEL=partial_LABEL+label\n","    print('New shape Tensor ', partial_TENSOR.shape)\n","    print('New shape label ', len(partial_LABEL))\n","    time2 = datetime.datetime.now()\n","    elapsedTime = time2 - time1\n","    print('Minutes it took',elapsedTime)\n","        \n","    time01 = datetime.datetime.now()\n","    elapsedTime0 = time01 - time00\n","    print('Total time',elapsedTime0)\n","\n","    return(partial_TENSOR, partial_LABEL) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQNacYQOLf4P"},"source":["import random\n","import numpy as np\n","\n","def random_shuffle(FINAL_TENSOR, labels):\n","    random_indexes_tens= random.sample(range(0, FINAL_TENSOR.shape[0]), FINAL_TENSOR.shape[0])\n","    event_tensors= [t for t in range(FINAL_TENSOR.shape[0])]#indexes of tensor\n","    \n","    #print(random_indexes_tens)\n","    #print(event_tensors)\n","\n","    SHUFFLED_TENSOR=[] \n","    LABELS_SHUFFLED=[]\n","    \n","    for i in event_tensors:\n","        target_position=random_indexes_tens[i]\n","        SHUFFLED_TENSOR.append(FINAL_TENSOR[target_position,:,:])\n","        LABELS_SHUFFLED.append(labels[target_position])\n","\n","    SHUFFLED_TENSOR=numpy.array(SHUFFLED_TENSOR)\n","    #SHUFFLED_TENSOR= np.transpose(SHUFFLED_TENSOR, (1,2,0))\n","    return SHUFFLED_TENSOR, LABELS_SHUFFLED"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppAwtqVVLf4P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625085524387,"user_tz":-120,"elapsed":3045,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"491d4085-a663-462a-849e-c82387476b67"},"source":["### Here we set our CNN\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","def run_model():\n","  nb_channels=58\n","  window_size=W1+W2\n","  model = Sequential()\n","  seed_value=333        \n","    \n","    #####\n","  model.add(Conv2D(filters = 64, kernel_size = (7,7), padding = \"same\", activation = \"elu\", input_shape = (nb_channels,window_size,1)))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size = (3,3)))\n","  model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = \"same\", activation = \"elu\"))\n","#model.add(Activation('elu'))# Is this needed? Try without it\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size = (3,3)))\n","  model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"elu\"))\n","#model.add(Activation('elu'))#Is this needed? Try without it\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size = (3,3)))\n","  model.add(Flatten())\n","  model.add(Dropout(0.2)) #seed=seed_value)\n","  model.add(Dense(32, activation = \"elu\"))\n","  #model.add(Activation('elu'))#Is this needed? Try without it\n","  model.add(BatchNormalization())\n","#model.add(Dropout(0.2))\n","  model.add(Dense(1, activation = \"sigmoid\"))#try softax\n","\n","\n","  adam = Adam(lr = 0.0001)\n","\n","  model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = ['accuracy','mse']) #tf.math.reduce_std(x, 1), ['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'cosine_proximity']\n","# simple early stopping\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)# add \"patience=200\" if early stopping is done too soon\n","  return model, es, adam\n","model, es, adam= run_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Kpt88UDuaI-d"},"source":["with open(\"model summary \"+str(BIG_TITLE)+'.pdf', 'w') as f:\n","  model.summary(print_fn=lambda x: f.write(x + '\\n'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGyExfvoNrEl","executionInfo":{"status":"ok","timestamp":1625085524835,"user_tz":-120,"elapsed":269,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"71f00a55-c146-433d-ed1f-efcf7506b1c8"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 58, 768, 64)       3200      \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 58, 768, 64)       256       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 19, 256, 64)       0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 19, 256, 64)       102464    \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 19, 256, 64)       256       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 6, 85, 64)         0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 6, 85, 64)         36928     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 6, 85, 64)         256       \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 2, 28, 64)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3584)              0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 3584)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 32)                114720    \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 32)                128       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 258,241\n","Trainable params: 257,793\n","Non-trainable params: 448\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RlvLmhg9e8sv"},"source":["import matplotlib.pyplot as pyplot\n","import csv\n","\n","def plot_training_history(history, TITLE):\n","    acc = history.history['accuracy'] #this does not work, to do\n","    loss = history.history['loss']\n","    mse = history.history['mse']\n","    epochs = range(len(acc))\n"," \n","    plt.figure(figsize=(15, 6))\n","    plt.subplot(1, 1, 1)\n","    plt.plot(epochs, acc, 'g', label='Training accuracy',linewidth=2) #'g'\n","    plt.plot(epochs, loss, 'r', label='Training loss',linewidth=2)\n","    plt.plot(epochs, mse, 'b--', label='MSE',linewidth=2)\n","    #plt.plot(epochs, ACC, 'g', label='Test acc',linewidth=2)\n","    plt.title(TITLE, fontsize=10)\n","    plt.xlabel('Number of epochs', fontsize=8)\n","    plt.ylabel('Performance indicator', fontsize=8)\n","    plt.legend()\n","    \n","    plt.savefig(str(TITLE)+'.png', bbox_inches='tight')\n","    plt.savefig(str(TITLE)+'.pdf', bbox_inches='tight')\n","    #plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFKES68afCVt","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"14MNvvldL0zRJeNuiCta75YHV96OMVAxo"},"executionInfo":{"status":"ok","timestamp":1625101449782,"user_tz":-120,"elapsed":137473,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"9115f74b-aeab-40f9-baf2-326bf4ee948d"},"source":["#Training of CNN\n","# .....................\n","nb_channels=58\n","window_size=W1+W2\n","training_accuracies=[]\n","test_accuracies=[]\n","files= ['01','02','03','04','05','06','07','08','09','10','11','12','13','14', '15']#LEAVE ONE OUT 15, for test\n","accuracies={}\n","histories={}\n","training_time=[]\n","\n","for g in range(len(files)):\n","  train_files= [files[:g]+ files[g+1:]]\n","  test_file=[files[g]]\n","  model, es, adam=run_model()\n","\n","  for file in files:\n","    \n","    ###\n","      REACH_TENSOR, REACH_LABELS = add_data(W1=W1, W2=W2,k=file,event_label=['REACH'],EVENT=503587,nb_channels=nb_channels)\n","    \n","      REACH_TENSOR2, REACH_LABELS2 = add_data(W1=W1, W2=W2,k=file,\n","                                          event_label=['REACH'],EVENT=503588,nb_channels=nb_channels)\n","    \n","      GRASP_TENSOR, GRASP_LABELS = add_data(W1=W1, W2=W2,k=file,\n","                                          event_label=['GRASP'],EVENT=501794,nb_channels=nb_channels) \n","      GRASP_TENSOR2, GRASP_LABELS2 = add_data(W1=W1, W2=W2,k=file,\n","                                          event_label=['GRASP'],EVENT=501795,nb_channels=nb_channels)\n","    \n","      try:\n","          FINAL_TENSOR=numpy.vstack((FINAL_TENSOR,REACH_TENSOR,REACH_TENSOR2, GRASP_TENSOR, GRASP_TENSOR2))\n","          labels=labels+REACH_LABELS+REACH_LABELS2+GRASP_LABELS+GRASP_LABELS2\n","        \n","      except NameError:\n","          FINAL_TENSOR=numpy.vstack((REACH_TENSOR,REACH_TENSOR2, GRASP_TENSOR, GRASP_TENSOR2))\n","          labels=REACH_LABELS+REACH_LABELS2+GRASP_LABELS+GRASP_LABELS2\n","\n","  X_train, X_val, y_train, y_val = train_test_split(FINAL_TENSOR, labels, test_size =0.3, random_state=0)\n","\n","  X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n","  X_val=X_val.reshape(X_val.shape[0],X_val.shape[1],X_val.shape[2],1)\n","\n","  print(\"x train shape\", X_train.shape)\n","  print(\"x val shape\", X_val.shape)\n","  print(\"y train length\", len(y_train))\n","  print(\"y train length\", len(y_val))\n","\n","  del FINAL_TENSOR\n","  del REACH_TENSOR \n","  del REACH_LABELS\n","  del REACH_TENSOR2 \n","  del REACH_LABELS2\n","  del GRASP_TENSOR\n","  del GRASP_LABELS\n","  del GRASP_TENSOR2\n","  del GRASP_LABELS2 \n","\n","  onehot = LabelBinarizer()\n","  y_train = onehot.fit_transform(y_train)\n","  y_val = onehot.transform(y_val)\n","  ####\n","  start = time.time()\n","  TITLE= \"Training accuracy, loss, and MSE for \"+BIG_TITLE+ \"K-fold number\"+ str(g)\n","  histories[TITLE]= model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=10, callbacks=[es])#works with epochs = 10\n","  stop = time.time() \n","\n","  training_accuracies.append(histories[TITLE].history['accuracy'])\n","  training_time.append(stop - start)\n","\n","  plot_training_history(histories[TITLE], TITLE)\n","\n","  fig = plt.figure()\n","  pyplot.plot(histories[TITLE].history['loss'], label='Train loss')\n","  pyplot.plot(histories[TITLE].history['val_loss'], label='Validation loss')\n","  TITLE= \"History train loss and validation loss \"+BIG_TITLE+ \"K-fold number\"+ str(g)\n","\n","  fig.suptitle(TITLE, fontsize=10)\n","  plt.xlabel('Number of epochs', fontsize=8)\n","  plt.ylabel('Loss', fontsize=8)\n","\n","  pyplot.legend(loc='upper center')\n","\n","  plt.savefig(str(TITLE)+'.png', bbox_inches='tight')\n","  plt.savefig(str(TITLE)+'.pdf', bbox_inches='tight')\n","\n","  del X_train\n","  del X_val\n","  del y_train\n","  del y_val\n","\n","###################################\n","  for file in test_file:\n","    REACH_TENSOR, REACH_LABELS = add_data(W1=W1, W2=W2,k=file,event_label=['REACH'],EVENT=503587,nb_channels=nb_channels)\n","    \n","    REACH_TENSOR2, REACH_LABELS2 = add_data(W1=W1, W2=W2,k=file,\n","                                         event_label=['REACH'],EVENT=503588,nb_channels=nb_channels)\n","    \n","    GRASP_TENSOR, GRASP_LABELS = add_data(W1=W1, W2=W2,k=file,\n","                                         event_label=['GRASP'],EVENT=501794,nb_channels=nb_channels) \n","    GRASP_TENSOR2, GRASP_LABELS2 = add_data(W1=W1, W2=W2,k=file,\n","                                         event_label=['GRASP'],EVENT=501795,nb_channels=nb_channels)\n","    try:\n","        TEST_TENSOR=numpy.vstack((TEST_TENSOR,REACH_TENSOR,REACH_TENSOR2, GRASP_TENSOR, GRASP_TENSOR2))\n","        labels_test=labels_test+REACH_LABELS+REACH_LABELS2+GRASP_LABELS+GRASP_LABELS2\n","        \n","    except NameError:\n","        TEST_TENSOR=numpy.vstack((REACH_TENSOR,REACH_TENSOR2, GRASP_TENSOR, GRASP_TENSOR2))\n","        labels_test=REACH_LABELS+REACH_LABELS2+GRASP_LABELS+GRASP_LABELS2\n","        \n","\n","  TEST_TENSOR, labels_test=random_shuffle(TEST_TENSOR, labels_test)\n","  TEST_TENSOR=TEST_TENSOR.reshape(TEST_TENSOR.shape[0],TEST_TENSOR.shape[1],TEST_TENSOR.shape[2],1)\n","  y_test = onehot.transform(labels_test)\n","  print(\"TEST_TENSOR shape:\", TEST_TENSOR.shape)\n","  print(\"y test length:\", len(y_test))\n","\n","  test_loss, test_acc, test_mse = model.evaluate(TEST_TENSOR, y_test)\n","  test_accuracies.append(test_acc)\n","\n","##storing for ROC curve\n","  y_pred = model.predict(TEST_TENSOR)\n","  y_pred_keras=y_pred.ravel()\n","  fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\n","  ROC= [\"fpr_keras:\", fpr_keras, \"tpr_keras :\", tpr_keras, \"thresholds_keras:\", thresholds_keras]\n","##storing for AUC curve\n","  auc_keras = auc(fpr_keras, tpr_keras)\n","  AUC= [\"auc_keras:\", auc_keras]\n","\n","# Creating the Confusion Matrix\n","  y_pred_n=[]\n","  for i in y_pred:\n","    if i >= 0.5:\n","      y_pred_n.append(1)\n","    else:\n","      y_pred_n.append(0)\n","  y_pred= y_pred_n\n","#del y_pred_n\n","  cm = confusion_matrix(y_test, y_pred)\n","  print(cm)\n","\n","  total1=sum(sum(cm))\n","#####from confusion matrix calculate accuracy\n","  accuracy1=(cm[0,0]+cm[1,1])/float(total1)\n","  acc=\"Accuracy : %f\" %(accuracy1*100)\n","  print(\"Accuracy : %f\" %(accuracy1*100))\n","\n","  sensitivity1 = float(cm[0,0])/(cm[0,0]+cm[0,1])\n","  sens=\"Sensitivity :%f\" %(sensitivity1*100)\n","  print(\"Sensitivity :%f\" %(sensitivity1*100))\n","\n","  specificity1 = float(cm[1,1])/(cm[1,0]+cm[1,1])\n","  spe=\"Specificity :%f\"%(specificity1*100)\n","  print(\"Specificity :%f\"%(specificity1*100))\n","\n","  precision1 = float(cm[0,0])/(cm[0,0]+cm[1,1])\n","  pre=\"Precision :%f\" %(precision1*100)\n","  print(\"Precision :%f\" %(precision1*100))\n","\n","  F1score= 2*((precision1*sensitivity1)/(precision1+sensitivity1))\n","\n","  TITLE= \"Test results & training time \"+BIG_TITLE+ \"K-fold number\"+ str(g)\n","\n","\n","  with open(str(TITLE)+\".csv\", 'w', newline='', ) as file:\n","      writer = csv.writer(file)\n","      writer.writerow([\"Test acc:\", test_acc])\n","      writer.writerow([\"Test loss:\", test_loss])\n","      writer.writerow([\"Test mse:\", test_mse])\n","      writer.writerow([\"ROC:\", ROC])\n","      writer.writerow([\"AUC:\", AUC])\n","      writer.writerow([\"CM:\",cm])\n","      writer.writerow([\"accuracy:\",acc])\n","      writer.writerow([\"sensitivity/recall:\", sens])\n","      writer.writerow([\"specificity:\", spe])\n","      writer.writerow([\"precision\",pre])\n","      writer.writerow([\"Training time:\", training_time])\n","\n","  del TEST_TENSOR\n","  del labels_test\n","  del y_test\n","  del y_pred\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"7ljDE9KFfFu-"},"source":["import math\n","\n","def variance(data, ddof=0):\n","  n = len(data)\n","  mean = sum(data) / n\n","  return sum((x - mean) ** 2 for x in data) / (n - ddof)\n","\n","def stdev(data):\n","  var = variance(data)\n","  std_dev = math.sqrt(var)\n","  return std_dev\n","\n","def Average(lst):\n","    return sum(lst) / len(lst)\n","\n","SE_test= stdev(test_accuracies)\n","Mean_accuracy= Average(test_accuracies)\n","\n","with open(str(BIG_TITLE+\"SE\")+\".csv\", 'w', newline='', ) as file:\n","      writer = csv.writer(file)\n","      writer.writerow([\"SE_test\", SE_test])\n","      writer.writerow([\"Mean_accuracy\", Mean_accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoIwT3DJfCYY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625101452527,"user_tz":-120,"elapsed":1242,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"76582bb6-19f4-4aa4-b9cc-e1f4dfa0d0d4"},"source":["print(SE_test*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5.651825485793478\n","5.651825485793478\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"82hYJ1xte80g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625101452530,"user_tz":-120,"elapsed":1040,"user":{"displayName":"Nicolas Aguiar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj2Ud0h4Qt1JKXD1XCrhkA-k-9ceXcQU91l1gp=s64","userId":"14782219282177773206"}},"outputId":"9b3e1696-c917-4fa4-c635-444a3d315583"},"source":["print(Mean_accuracy*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["86.91279927889506\n","86.91279927889506\n"],"name":"stdout"}]}]}