{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SVM movement EEG spectral features.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BClhnfFo0Fl",
        "outputId": "e49266e4-0b84-4332-fd41-9dbebc5ac7a0"
      },
      "source": [
        "## The following is for google colab\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCb6gLiTOIzc",
        "outputId": "b7143666-dcf8-42c1-d07b-f9e69640bf74"
      },
      "source": [
        "## The following is for google colab\n",
        "%cd /content/drive/My\\ Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVL7HFloOFFD"
      },
      "source": [
        "BIG_TITLE=\"SVM movement\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN_jGN1AQj_q"
      },
      "source": [
        "### TIME WINDOWS\n",
        "w1=512\n",
        "w2=768\n",
        "W1=w1\n",
        "W2=w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeGnjhxHOiXE"
      },
      "source": [
        "### LIBRARIES\n",
        "\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import datetime\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import pandas\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Remove some unwanted warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True \n",
        "from scipy.signal import butter, lfilter\n",
        "import pywt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import scipy\n",
        "import scipy.signal\n",
        "from scipy import fft\n",
        "### LIBRARIES\n",
        "\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import datetime\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import scipy\n",
        "import pandas\n",
        "import os\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "import pywt\n",
        "import scipy.io as spio\n",
        "from scipy.stats import entropy\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import timeit\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from numpy import hstack, vstack, dstack\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inj1meYto0Fm"
      },
      "source": [
        "def load_signal(k):  #where k is the file name\n",
        "    data = sio.loadmat(str(k))  \n",
        "    ###deleting EOGs\n",
        "    signal_raw= numpy.delete(data[\"signal\"], obj= [63,62,61,60,59,58], axis=0)\n",
        "    ###deleting 0s:\n",
        "    signal_raw_trans= numpy.transpose(signal_raw)\n",
        "    ind=[]\n",
        "    for cx, c in enumerate(signal_raw_trans):\n",
        "        if len(c) -  numpy.count_nonzero(c) <=5:\n",
        "            continue\n",
        "        else:\n",
        "            ind.append(cx)\n",
        "    g= numpy.delete(signal_raw_trans, obj= ind, axis=0)\n",
        "    signal_raw_trans_clean=numpy.transpose(g)\n",
        "    \n",
        "    ###clean memory\n",
        "    del data\n",
        "    del signal_raw\n",
        "    del signal_raw_trans\n",
        "    del g\n",
        "    return(signal_raw_trans_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePWzEAYo0Fm"
      },
      "source": [
        "### cleaned code names\n",
        "def cleaned_code_names(k):# k=file name\n",
        "    data = sio.loadmat(str(k)) \n",
        "    events= data[\"header\"][\"event_codes\"]\n",
        "    EVENTS=[]\n",
        "    for i in events:\n",
        "        for e in i:\n",
        "            for j in e:\n",
        "                for h in j:\n",
        "                    if len(str(h))>2:\n",
        "                        EVENTS.append(h)\n",
        "    EVENTS=numpy.array(EVENTS)\n",
        "    \n",
        "    del data\n",
        "    del events\n",
        "    \n",
        "    return EVENTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4dC4HPbo0Fn"
      },
      "source": [
        "### cleaned event code\n",
        "def event_Code(k): # k=file name\n",
        "    data = sio.loadmat(str(k)) \n",
        "    labels= str(data[\"events\"][\"codes\"]).split()\n",
        "    \n",
        "    l=[]\n",
        "    for g in labels:\n",
        "        for i in g:\n",
        "            if i.isnumeric():\n",
        "                l.append(i)\n",
        "        else:\n",
        "            l.append(\",\")\n",
        "    #cleaning labels \n",
        "    l2=[]\n",
        "    g=\"\"\n",
        "    for j in l:\n",
        "        if j.isnumeric():\n",
        "            g+=j\n",
        "        elif len(g)<1:\n",
        "            continue\n",
        "        else:\n",
        "            l2.append(int(g))\n",
        "            g=\"\"\n",
        "    del data\n",
        "    del labels\n",
        "    del l\n",
        "    del g\n",
        "    \n",
        "    return l2\n",
        "\n",
        "### cleaned event position\n",
        "def event_Position(k):# k=file name\n",
        "    data = sio.loadmat(str(k)) \n",
        "    indices_signal_matrix= str(data[\"events\"][\"positions\"]).split()\n",
        "    #cleaning indices_signal_matrix \n",
        "    l3=[]\n",
        "    for g in indices_signal_matrix:\n",
        "        for i in g:\n",
        "            if i.isnumeric():\n",
        "                l3.append(i)\n",
        "        else:\n",
        "            l3.append(\",\")\n",
        "    l4=[]\n",
        "    g=\"\"\n",
        "    for j in l3:\n",
        "        if j.isnumeric():\n",
        "            g+=j\n",
        "        elif len(g)<1:\n",
        "            continue\n",
        "        else:\n",
        "            l4.append(int(g))\n",
        "            g=\"\"\n",
        "    l4=l4[:-1]\n",
        "    del data\n",
        "    del indices_signal_matrix\n",
        "    del l3\n",
        "    del g\n",
        "    \n",
        "    return l4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jix8a1Xo0Fn"
      },
      "source": [
        "#indexes of each events within signals\n",
        "def indexes_event_signals(k):\n",
        "    data = sio.loadmat(str(k)) \n",
        "    EVENTS = cleaned_code_names(k)\n",
        "    event_codes= event_Code(k)\n",
        "    dico_indexes={}\n",
        "    for event in EVENTS:\n",
        "        indexes_=[]\n",
        "        for ig, g in enumerate(event_codes):\n",
        "            if g == event:\n",
        "                indexes_.append(ig)\n",
        "        dico_indexes[event]= indexes_\n",
        "    del data\n",
        "    del EVENTS\n",
        "    del event_codes\n",
        "    del indexes_\n",
        "    return dico_indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBmBAtj4o0Fo"
      },
      "source": [
        "def start_indexes(k, event):\n",
        "    indexes=indexes_event_signals(k)\n",
        "    event_positions= event_Position(k)\n",
        "    event_indexes=[]\n",
        "    for h in indexes[event]:\n",
        "        event_indexes.append(event_positions[h])\n",
        "    del indexes\n",
        "    del event_positions\n",
        "    return event_indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWtiwEHqo0Fo"
      },
      "source": [
        "def EVENT_SIGNALS(k, EVENT, W1, W2):\n",
        "    signal= load_signal(str(k)) \n",
        "    event_indexs= start_indexes(str(k), EVENT)\n",
        "    movement_signals=[]\n",
        "    for i in event_indexs:#\n",
        "        for channel in signal:\n",
        "            i= int(i)\n",
        "            if i-W1>0:\n",
        "                movement_signals.append(numpy.array(channel[i-W1:i+W2]))\n",
        "            else:\n",
        "                continue\n",
        "    del signal\n",
        "    del event_indexs\n",
        "    \n",
        "    return movement_signals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7r_8Ys5o0Fp",
        "scrolled": true
      },
      "source": [
        "def create_tensor(data,W1:int, W2,nb_channels:int):\n",
        "    window_size= W1+W2\n",
        "    positions_split=[]\n",
        "    for j in range(0,len(data),nb_channels):\n",
        "        positions_split.append(j)\n",
        "    positions_split.append(len(data))\n",
        "    all_events=[]\n",
        "    for i in range(0,len(positions_split)-1):\n",
        "        one_event=[]\n",
        "        for h in range(positions_split[i],positions_split[i+1]):\n",
        "            data_shaped=data[h]\n",
        "            if (len(one_event)==0.0):\n",
        "                one_event=data_shaped\n",
        "            else:\n",
        "                one_event=numpy.vstack((one_event, data_shaped))\n",
        "        if (len(all_events)==0.0):\n",
        "                all_events=one_event.reshape(1,nb_channels,window_size)\n",
        "        else:\n",
        "                all_events=numpy.vstack((all_events, one_event.reshape(1,nb_channels,window_size)))\n",
        "    return(all_events)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDL4BmC0o0Fq"
      },
      "source": [
        "def random_shuffle(final_TENSOR, labels):\n",
        "    random_indexes_tens= random.sample(range(0, final_TENSOR.shape[0]), final_TENSOR.shape[0])\n",
        "    event_tensors= [t for t in range(final_TENSOR.shape[0])]#indexes of tensor\n",
        "    SHUFFLED_TENSOR=[] \n",
        "    LABELS_SHUFFLED=[]\n",
        "    \n",
        "    for i in event_tensors:\n",
        "        target_position=random_indexes_tens[i]\n",
        "        SHUFFLED_TENSOR.append(final_TENSOR[target_position,:,:])\n",
        "        LABELS_SHUFFLED.append(labels[target_position])\n",
        "    SHUFFLED_TENSOR=numpy.array(SHUFFLED_TENSOR)\n",
        "    return SHUFFLED_TENSOR, LABELS_SHUFFLED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCYb9H1Mo0Fq"
      },
      "source": [
        "\n",
        "def add_data(W1:int, W2:int,k:str,event_label:str,EVENT,nb_channels:int):\n",
        "    #window_size=W1+W2\n",
        "    partial_TENSOR=[]\n",
        "    partial_LABEL=[]\n",
        "    time00 = datetime.datetime.now()\n",
        "    #for i in k:\n",
        "    time1 = datetime.datetime.now()\n",
        "    file='G'+k+'.mat'\n",
        "    print('processing '+file)\n",
        "    data=EVENT_SIGNALS(k=file, EVENT=EVENT, W1=W1, W2=W2 )\n",
        "    data_transformed=create_tensor(data=data,W1=W1, W2=W2,nb_channels=nb_channels)\n",
        "    label= event_label*data_transformed.shape[0]\n",
        "    \n",
        "    if (len(partial_TENSOR)==0.0):\n",
        "        partial_TENSOR=data_transformed\n",
        "        partial_LABEL=label\n",
        "    else:\n",
        "        partial_TENSOR=numpy.vstack((partial_TENSOR, data_transformed))\n",
        "        partial_LABEL=partial_LABEL+label\n",
        "    print('New shape Tensor ', partial_TENSOR.shape)\n",
        "    print('New shape label ', len(partial_LABEL))\n",
        "    time2 = datetime.datetime.now()\n",
        "    elapsedTime = time2 - time1\n",
        "    print('Minutes it took',elapsedTime)\n",
        "        \n",
        "    time01 = datetime.datetime.now()\n",
        "    elapsedTime0 = time01 - time00\n",
        "    print('Total time',elapsedTime0)\n",
        "\n",
        "    return(partial_TENSOR, partial_LABEL) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX6kWYVlo0Fs"
      },
      "source": [
        "data = sio.loadmat(str(\"G01.mat\"))\n",
        "EEG_signals= load_signal(\"G01.mat\")\n",
        "header_channels_labels= data[\"header\"][\"channels_labels\"]#living out EOGs\n",
        "header_channels_labels_filtered=[]\n",
        "for i in header_channels_labels:\n",
        "    for j in i:\n",
        "        for s in j:\n",
        "            for ij in s:\n",
        "                for ix in ij:\n",
        "                    header_channels_labels_filtered.append(str(ix).strip())\n",
        "header_channels_labels_filtered=header_channels_labels_filtered[:-6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y10ByYj4o0Fs"
      },
      "source": [
        "#band pass filter between 0.5 and 60 hz\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3XB_2F_o0Fv"
      },
      "source": [
        "ch=58\n",
        "nb_ch=58\n",
        "fs = 256.0\n",
        "lowcut = 0.5\n",
        "highcut = 60.0\n",
        "a = 0.02\n",
        "f0 = 256.0\n",
        "T = 1.0 / 256.0\n",
        "w = pywt.Wavelet('db4')\n",
        "\n",
        "def get_Filter_FeatExtract(file,W1,W2, nb_ch):\n",
        "    P1_reach= EVENT_SIGNALS(file, EVENT= 503587, W1=W1, W2=W2)\n",
        "    P1_reach2=EVENT_SIGNALS(file, EVENT= 503588, W1=W1, W2=W2)\n",
        "    P1_grasp= EVENT_SIGNALS(file, EVENT= 501794, W1=W1, W2=W2)\n",
        "    P1_grasp2=EVENT_SIGNALS(file, EVENT= 501795, W1=W1, W2=W2)\n",
        "    \n",
        "    P1_reach= create_tensor(P1_reach,W1=W1, W2=W2,nb_channels=nb_ch)\n",
        "    P1_reach2=create_tensor(P1_reach2,W1=W1, W2=W2,nb_channels=nb_ch)\n",
        "    P1_grasp=create_tensor(P1_grasp,W1=W1, W2=W2,nb_channels=nb_ch)\n",
        "    P1_grasp2=create_tensor(P1_grasp2,W1=W1, W2=W2,nb_channels=nb_ch)\n",
        "         \n",
        "\n",
        "    for event_i in range(0,P1_reach.shape[0]):  \n",
        "      y_r = butter_bandpass_filter(P1_reach[event_i], lowcut, highcut, fs, order=6)\n",
        "      #array_r = Implement_Notch_Filter(T, 1, 60, 1, 2, 'butter', y_r)\n",
        "      maxlev = pywt.dwt_max_level(len(y_r), w.dec_len)\n",
        "      # maxlev = 2 # Override if desired\n",
        "      #print(\"maximum level is \" + str(maxlev)) #max level is 3\n",
        "      threshold = 0.04 #\n",
        "      data= y_r\n",
        "      coeffs = pywt.wavedec(data, 'db4', level=maxlev)# \n",
        "      datarec = pywt.waverec(coeffs, 'db4')\n",
        "      try:\n",
        "        partial_reachF=numpy.vstack((partial_reachF,datarec.reshape(1, datarec.shape[0], datarec.shape[1])))\n",
        "      except NameError:\n",
        "        partial_reachF=datarec.reshape(1, datarec.shape[0], datarec.shape[1])\n",
        "\n",
        "    del y_r\n",
        "    del P1_reach\n",
        "\n",
        "    for event_i in range(0,P1_reach2.shape[0]): \n",
        "      y_r = butter_bandpass_filter(P1_reach2[event_i], lowcut, highcut, fs, order=6)\n",
        "      #array_r = Implement_Notch_Filter(T, 1, 60, 1, 2, 'butter', y_r)\n",
        "      maxlev = pywt.dwt_max_level(len(y_r), w.dec_len)\n",
        "      # maxlev = 2 # Override if desired\n",
        "      threshold = 0.04 #\n",
        "      data= y_r\n",
        "      coeffs = pywt.wavedec(data, 'db4', level=maxlev)# \n",
        "      datarec = pywt.waverec(coeffs, 'db4')\n",
        "      try:\n",
        "        partial_reachFF=numpy.vstack((partial_reachFF,datarec.reshape(1, datarec.shape[0], datarec.shape[1])))\n",
        "      except NameError:\n",
        "        partial_reachFF=datarec.reshape(1, datarec.shape[0], datarec.shape[1])\n",
        "\n",
        "    del y_r\n",
        "    del P1_reach2\n",
        "\n",
        "    for event_i in range(0,P1_grasp.shape[0]):  \n",
        "      y_r = butter_bandpass_filter(P1_grasp[event_i], lowcut, highcut, fs, order=6)\n",
        "      maxlev = pywt.dwt_max_level(len(y_r), w.dec_len)\n",
        "      # maxlev = 2 # Override if desired\n",
        "      threshold = 0.04 #\n",
        "      data= y_r\n",
        "      coeffs = pywt.wavedec(data, 'db4', level=maxlev)# \n",
        "      datarec = pywt.waverec(coeffs, 'db4')\n",
        "      try:\n",
        "        partial_graspF=numpy.vstack((partial_graspF,datarec.reshape(1, datarec.shape[0], datarec.shape[1])))\n",
        "      except NameError:\n",
        "        partial_graspF=datarec.reshape(1, datarec.shape[0], datarec.shape[1])\n",
        "\n",
        "    del y_r\n",
        "    del P1_grasp\n",
        "\n",
        "    for event_i in range(0,P1_grasp2.shape[0]): \n",
        "      y_r = butter_bandpass_filter(P1_grasp2[event_i], lowcut, highcut, fs, order=6)\n",
        "      #array_r = Implement_Notch_Filter(T, 1, 60, 1, 2, 'butter', y_r)\n",
        "      maxlev = pywt.dwt_max_level(len(y_r), w.dec_len)\n",
        "      # maxlev = 2 # Override if desired\n",
        "      #print(\"maximum level is \" + str(maxlev)) #max level is 3\n",
        "      threshold = 0.04 #\n",
        "      data= y_r\n",
        "      coeffs = pywt.wavedec(data, 'db4', level=maxlev)# \n",
        "      datarec = pywt.waverec(coeffs, 'db4')\n",
        "      try:\n",
        "        partial_graspFF=numpy.vstack((partial_graspFF,datarec.reshape(1, datarec.shape[0], datarec.shape[1])))\n",
        "      except NameError:\n",
        "        partial_graspFF=datarec.reshape(1, datarec.shape[0], datarec.shape[1])\n",
        "\n",
        "    del y_r\n",
        "    del P1_grasp2\n",
        "    \n",
        "\n",
        "    partial_reach= numpy.vstack([partial_reachF, partial_reachFF])\n",
        "    labels_reach= [\"reach\"]*partial_reach.shape[0]\n",
        "    partial_grasp=numpy.vstack([partial_graspF, partial_graspFF])\n",
        "    labels_grasp=[\"grasp\"]*partial_grasp.shape[0]\n",
        "    return partial_reach, labels_reach, partial_grasp, labels_grasp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsN-aXxe58uc"
      },
      "source": [
        "# Function to compute the classification using SVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def compute_SVC(train_f,train_l):\n",
        "\tC=1.0\n",
        "\tcache_size=200\n",
        "\tclass_weight=None\n",
        "\tcoef0=0.0\n",
        "\tdecision_function_shape=None\n",
        "\tdegree=3\n",
        "\tgamma='auto'\n",
        "\tkernel='rbf'\n",
        "\tmax_iter=-1\n",
        "\tprobability=False\n",
        "\trandom_state=None\n",
        "\tshrinking=True\n",
        "\ttol=0.001\n",
        "\tverbose=False\n",
        "\tc = svm.SVC(kernel='rbf')\n",
        "\tc.fit(train_f,train_l)\n",
        "\treturn c;\n",
        "\t\n",
        "# Function to calculate the accuracy\n",
        "def svm_results(test_f,test_l,c):\n",
        "  pred = c.predict(test_f)\n",
        "\t#print(pred)\n",
        "  Accuracy_Score = accuracy_score(test_l, pred)\n",
        "  Precision_Score = precision_score(test_l, pred,  average=\"macro\")\n",
        "  Recall_Score = recall_score(test_l, pred,  average=\"macro\")\n",
        "  F1_Score = f1_score(test_l, pred,  average=\"macro\")\n",
        "  CM = confusion_matrix(test_l, pred)\n",
        "  return Accuracy_Score, Precision_Score, Recall_Score, F1_Score, CM, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWvZGxI38au9"
      },
      "source": [
        "import functools\n",
        "def combine_dims(a, i=0, n=1):\n",
        "  \"\"\"\n",
        "  Combines dimensions of numpy array `a`, \n",
        "  starting at index `i`,\n",
        "  and combining `n` dimensions\n",
        "  \"\"\"\n",
        "  s = list(a.shape)\n",
        "  combined = functools.reduce(lambda x,y: x*y, s[i:i+n+1])\n",
        "  return np.reshape(a, s[:i] + [combined] + s[i+n+1:])\n",
        "\n",
        "#imgs = combine_dims(FINAL_TENSOR, 1)#combines dimension 1 and 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVE81u7no0Fx",
        "outputId": "8537137b-f1e5-468a-a712-6c4362404552"
      },
      "source": [
        "###\n",
        "nb_ch=58\n",
        "training_time=[]\n",
        "test_accuracies=[]\n",
        "#test_f1score=[]\n",
        "files= ['G01.mat','G02.mat','G03.mat','G04.mat','G05.mat','G06.mat','G07.mat','G08.mat','G09.mat','G10.mat','G11.mat','G12.mat','G13.mat','G14.mat','G15.mat']\n",
        "\n",
        "# .....................\n",
        "for g in range(len(files)):\n",
        "  train_files= files[:g]+ files[g+1:]\n",
        "  test_file=[files[g]]\n",
        "\n",
        "  for file in train_files:\n",
        "    partial_reach, labels_reach, partial_grasp, labels_grasp= get_Filter_FeatExtract(file,W1,W2, nb_ch)\n",
        "    beside= vstack((partial_reach,partial_grasp))\n",
        "    beside2=labels_reach+labels_grasp\n",
        "    try:\n",
        "      FINAL_TENSOR= vstack((FINAL_TENSOR, beside))\n",
        "      FINAL_LABELS= FINAL_LABELS+beside2\n",
        "    except NameError:\n",
        "      FINAL_TENSOR=beside\n",
        "      FINAL_LABELS=beside2\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(FINAL_TENSOR, FINAL_LABELS, test_size =0.3, random_state=0)\n",
        "  X_train=numpy.vstack((X_train,X_val))\n",
        "  print(\"x train shape\", X_train.shape)\n",
        "  y_train=y_train+y_val\n",
        "  print(\"y train length\", len(y_train))\n",
        "\n",
        "  del FINAL_TENSOR\n",
        "  del FINAL_LABELS\n",
        "  del beside\n",
        "  del beside2\n",
        "\n",
        "  X_train = combine_dims(X_train, 1)\n",
        "  onehot = LabelBinarizer()\n",
        "  y_train   = onehot.fit_transform(y_train)\n",
        "\n",
        "  from sklearn import svm\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  model_svc = compute_SVC(X_train, y_train.ravel())\n",
        "\n",
        "  stop = time.time()  \n",
        "  training_time.append(stop - start)\n",
        "\n",
        "# ### END TRAIN ##\n",
        "  del X_train\n",
        "  del X_val\n",
        "  del y_train\n",
        "  del y_val\n",
        "\n",
        "  for file in test_file:\n",
        "    partial_reach, labels_reach, partial_grasp, labels_grasp= get_Filter_FeatExtract(file,W1,W2, nb_ch)\n",
        "    besidE= vstack((partial_reach,partial_grasp))\n",
        "    besidE2=labels_reach+labels_grasp\n",
        "    try:\n",
        "      TEST_TENSOR= vstack((TEST_TENSOR, besidE))\n",
        "      TEST_LABELS= TEST_LABELS+besidE2\n",
        "    except NameError:\n",
        "      TEST_TENSOR=besidE\n",
        "      TEST_LABELS=besidE2\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(TEST_TENSOR, TEST_LABELS, test_size =0.3, random_state=0)\n",
        "  X_train=numpy.vstack((X_train,X_val))\n",
        "  print(\"x test shape before flattening\", X_train.shape)\n",
        "  y_train=y_train+y_val\n",
        "  print(\"y test length\", len(y_train))\n",
        "\n",
        "\n",
        "  X_train = combine_dims(X_train, 1)#combines dimension 1 and 2\n",
        "  print(\"x test shape after flattening\", len(X_train))#.shape)\n",
        "  onehot = LabelBinarizer()\n",
        "  y_train   = onehot.fit_transform(y_train)\n",
        "\n",
        "  from sklearn import svm\n",
        "\t\n",
        "  TAccuracy_Score, TPrecision_Score, TRecall_Score, TF1_Score, TCM, Tpred = svm_results(X_train,y_train,model_svc);\n",
        "\n",
        "  test_accuracies.append(TAccuracy_Score)\n",
        "  #test_f1score.append(TF1_Score)\n",
        "  \n",
        "  TITLE= \"Test results & training time \"+BIG_TITLE+ \"K-fold number \"+ str(g)\n",
        "  with open(str(TITLE)+\"K-fold number\"+ str(g)+\".csv\", 'w', newline='', ) as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow([\"Test acc:\", TAccuracy_Score])\n",
        "      writer.writerow([\"sensitivity/recall:\", TRecall_Score])\n",
        "      writer.writerow([\"CM:\", TCM])\n",
        "      writer.writerow([\"precision\",TPrecision_Score])#\n",
        "      #writer.writerow([\"F1-score\",TF1_Score])\n",
        "      #writer.writerow([\"Training time:\", training_time])\n",
        "\n",
        "\n",
        "  del TEST_TENSOR\n",
        "  del TEST_LABELS\n",
        "  del besidE      \n",
        "  del besidE2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x train shape (4338, 58, 1280)\n",
            "y train length 4338\n",
            "x test shape before flattening (316, 58, 1280)\n",
            "y test length 316\n",
            "x test shape after flattening 316\n",
            "x train shape (4354, 58, 1280)\n",
            "y train length 4354\n",
            "x test shape before flattening (300, 58, 1280)\n",
            "y test length 300\n",
            "x test shape after flattening 300\n",
            "x train shape (4338, 58, 1280)\n",
            "y train length 4338\n",
            "x test shape before flattening (316, 58, 1280)\n",
            "y test length 316\n",
            "x test shape after flattening 316\n",
            "x train shape (4352, 58, 1280)\n",
            "y train length 4352\n",
            "x test shape before flattening (302, 58, 1280)\n",
            "y test length 302\n",
            "x test shape after flattening 302\n",
            "x train shape (4334, 58, 1280)\n",
            "y train length 4334\n",
            "x test shape before flattening (320, 58, 1280)\n",
            "y test length 320\n",
            "x test shape after flattening 320\n",
            "x train shape (4336, 58, 1280)\n",
            "y train length 4336\n",
            "x test shape before flattening (318, 58, 1280)\n",
            "y test length 318\n",
            "x test shape after flattening 318\n",
            "x train shape (4346, 58, 1280)\n",
            "y train length 4346\n",
            "x test shape before flattening (308, 58, 1280)\n",
            "y test length 308\n",
            "x test shape after flattening 308\n",
            "x train shape (4334, 58, 1280)\n",
            "y train length 4334\n",
            "x test shape before flattening (320, 58, 1280)\n",
            "y test length 320\n",
            "x test shape after flattening 320\n",
            "x train shape (4342, 58, 1280)\n",
            "y train length 4342\n",
            "x test shape before flattening (312, 58, 1280)\n",
            "y test length 312\n",
            "x test shape after flattening 312\n",
            "x train shape (4346, 58, 1280)\n",
            "y train length 4346\n",
            "x test shape before flattening (308, 58, 1280)\n",
            "y test length 308\n",
            "x test shape after flattening 308\n",
            "x train shape (4336, 58, 1280)\n",
            "y train length 4336\n",
            "x test shape before flattening (318, 58, 1280)\n",
            "y test length 318\n",
            "x test shape after flattening 318\n",
            "x train shape (4336, 58, 1280)\n",
            "y train length 4336\n",
            "x test shape before flattening (318, 58, 1280)\n",
            "y test length 318\n",
            "x test shape after flattening 318\n",
            "x train shape (4342, 58, 1280)\n",
            "y train length 4342\n",
            "x test shape before flattening (312, 58, 1280)\n",
            "y test length 312\n",
            "x test shape after flattening 312\n",
            "x train shape (4376, 58, 1280)\n",
            "y train length 4376\n",
            "x test shape before flattening (278, 58, 1280)\n",
            "y test length 278\n",
            "x test shape after flattening 278\n",
            "x train shape (4346, 58, 1280)\n",
            "y train length 4346\n",
            "x test shape before flattening (308, 58, 1280)\n",
            "y test length 308\n",
            "x test shape after flattening 308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAymnTfKFsO7"
      },
      "source": [
        "import math\n",
        "\n",
        "def variance(data, ddof=0):\n",
        "  n = len(data)\n",
        "  mean = sum(data) / n\n",
        "  return sum((x - mean) ** 2 for x in data) / (n - ddof)\n",
        "\n",
        "def stdev(data):\n",
        "  var = variance(data)\n",
        "  std_dev = math.sqrt(var)\n",
        "  return std_dev\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "SE_testacc= stdev(test_accuracies)\n",
        "#SE_testf1= stdev(test_f1score)\n",
        "Mean_accuracy= Average(test_accuracies)\n",
        "#Mean_f1score= Average(test_f1score)\n",
        "\n",
        "with open(str(BIG_TITLE+\"SE\")+\".csv\", 'w', newline='', ) as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow([\"SE_test accuracy\", SE_testacc])\n",
        "      writer.writerow([\"Mean_accuracy\", Mean_accuracy])\n",
        "      #writer.writerow([\"SE_test f1score\", SE_testf1])\n",
        "      #writer.writerow([\"Mean_f1score\", Mean_f1score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w69_yS-YcLfU"
      },
      "source": [
        "print(\"SE acc\", SE_testacc*100)\n",
        "#print(\"SE F1\", SE_testf1*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j87c2LrkcNEi"
      },
      "source": [
        "print(\"Mean acc\", Mean_accuracy*100)\n",
        "#print(\"Mean F1 score\", Mean_f1score*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LPjCP1G8au_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefa2acf-3bc9-4d2a-9cfe-e53f0be349a4"
      },
      "source": [
        "test_accuracies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.629746835443038,\n",
              " 0.73,\n",
              " 0.6234177215189873,\n",
              " 0.6920529801324503,\n",
              " 0.590625,\n",
              " 0.5628930817610063,\n",
              " 0.6915584415584416,\n",
              " 0.621875,\n",
              " 0.6858974358974359,\n",
              " 0.7597402597402597,\n",
              " 0.710691823899371,\n",
              " 0.6037735849056604,\n",
              " 0.7115384615384616,\n",
              " 0.7553956834532374,\n",
              " 0.6590909090909091]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4QxMN66p053"
      },
      "source": [
        "#test_f1score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}